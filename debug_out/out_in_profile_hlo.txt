- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
selected_indices:  [[0, 1], [2, 3]]
selected_indices:  [[0, 1], [2, 3]]
- Compile all stages
- Profile all stages
----------------------------------------------------------forward-----------------------------------------------------------------
  input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
  output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
  donated_invars (False, False, False, False, False, False, False, False, False, False)
  hlo module: 
    HloModule stage_0_1-stage_0_1_acc_grad_0, 
    entry_computation_layout={
        (f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[64,2048]{1,0},f32[2048,8192]{1,0},f32[8192]{0},
        f32[8192,2048]{1,0},f32[2048]{0},f32[64,2048]{1,0})
        -->(f32[64,2048]{1,0}, pred[64,2048]{1,0}, f32[64,8192]{1,0}, pred[64,8192]{1,0}, f32[64,2048]{1,0}, /*index=5*/pred[64,2048]{1,0}, f32[64,8192]{1,0}, pred[64,8192]{1,0})}
  
  ENTRY %main.1125-stage_0_1_acc_grad_0_spmd (param.1: f32[2048,8192], param.2: f32[8192], param.3: f32[8192,2048], param.4: f32[2048], param: f32[64,2048], param.5: f32[2048,8192], param.6: f32[8192], param.7: f32[8192,2048], param.8: f32[2048], param.9: f32[64,2048]) -> (f32[64,2048], pred[64,2048], f32[64,8192], pred[64,8192], f32[64,2048], /*index=5*/pred[64,2048], f32[64,8192], pred[64,8192]) {
    %param = f32[64,2048]{1,0} parameter(4), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %param.1 = f32[2048,8192]{1,0} parameter(0), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot = f32[64,8192]{1,0} dot(f32[64,2048]{1,0} %param, f32[2048,8192]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.2 = f32[8192]{0} parameter(1), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.7 = f32[64,8192]{1,0} broadcast(f32[8192]{0} %param.2), dimensions={1}
    %add.0 = f32[64,8192]{1,0} add(f32[64,8192]{1,0} %dot, f32[64,8192]{1,0} %broadcast.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %constant.2 = f32[] constant(0)
    %broadcast.8 = f32[64,8192]{1,0} broadcast(f32[] %constant.2), dimensions={}
    %maximum.0 = f32[64,8192]{1,0} maximum(f32[64,8192]{1,0} %add.0, f32[64,8192]{1,0} %broadcast.8), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %param.3 = f32[8192,2048]{1,0} parameter(2), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.1 = f32[64,2048]{1,0} dot(f32[64,8192]{1,0} %maximum.0, f32[8192,2048]{1,0} %param.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.4 = f32[2048]{0} parameter(3), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.9 = f32[64,2048]{1,0} broadcast(f32[2048]{0} %param.4), dimensions={1}
    %add.1 = f32[64,2048]{1,0} add(f32[64,2048]{1,0} %dot.1, f32[64,2048]{1,0} %broadcast.9), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %broadcast.10 = f32[64,2048]{1,0} broadcast(f32[] %constant.2), dimensions={}
    %maximum.1 = f32[64,2048]{1,0} maximum(f32[64,2048]{1,0} %add.1, f32[64,2048]{1,0} %broadcast.10), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %compare.0 = pred[64,2048]{1,0} compare(f32[64,2048]{1,0} %add.1, f32[64,2048]{1,0} %broadcast.10), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %compare.1 = pred[64,8192]{1,0} compare(f32[64,8192]{1,0} %add.0, f32[64,8192]{1,0} %broadcast.8), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %param.5 = f32[2048,8192]{1,0} parameter(5), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.2 = f32[64,8192]{1,0} dot(f32[64,2048]{1,0} %maximum.1, f32[2048,8192]{1,0} %param.5), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.6 = f32[8192]{0} parameter(6), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.11 = f32[64,8192]{1,0} broadcast(f32[8192]{0} %param.6), dimensions={1}
    %add.2 = f32[64,8192]{1,0} add(f32[64,8192]{1,0} %dot.2, f32[64,8192]{1,0} %broadcast.11), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %maximum.2 = f32[64,8192]{1,0} maximum(f32[64,8192]{1,0} %add.2, f32[64,8192]{1,0} %broadcast.8), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    %param.7 = f32[8192,2048]{1,0} parameter(7), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.3 = f32[64,2048]{1,0} dot(f32[64,8192]{1,0} %maximum.2, f32[8192,2048]{1,0} %param.7), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.8 = f32[2048]{0} parameter(8), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.12 = f32[64,2048]{1,0} broadcast(f32[2048]{0} %param.8), dimensions={1}
    %add.3 = f32[64,2048]{1,0} add(f32[64,2048]{1,0} %dot.3, f32[64,2048]{1,0} %broadcast.12), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %maximum.3 = f32[64,2048]{1,0} maximum(f32[64,2048]{1,0} %add.3, f32[64,2048]{1,0} %broadcast.10), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %param.9 = f32[64,2048]{1,0} parameter(9), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %subtract.0 = f32[64,2048]{1,0} subtract(f32[64,2048]{1,0} %maximum.3, f32[64,2048]{1,0} %param.9), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %constant.14 = f32[] constant(2)
    %broadcast.13 = f32[64,2048]{1,0} broadcast(f32[] %constant.14), dimensions={}
    %multiply.0 = f32[64,2048]{1,0} multiply(f32[64,2048]{1,0} %subtract.0, f32[64,2048]{1,0} %broadcast.13), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %compare.2 = pred[64,2048]{1,0} compare(f32[64,2048]{1,0} %add.3, f32[64,2048]{1,0} %broadcast.10), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %compare.3 = pred[64,8192]{1,0} compare(f32[64,8192]{1,0} %add.2, f32[64,8192]{1,0} %broadcast.8), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    ROOT %tuple = (f32[64,2048]{1,0}, pred[64,2048]{1,0}, f32[64,8192]{1,0}, pred[64,8192]{1,0}, f32[64,2048]{1,0}, /*index=5*/pred[64,2048]{1,0}, f32[64,8192]{1,0}, pred[64,8192]{1,0}) tuple(f32[64,2048]{1,0} %maximum.1, pred[64,2048]{1,0} %compare.0, f32[64,8192]{1,0} %maximum.0, pred[64,8192]{1,0} %compare.1, f32[64,2048]{1,0} %multiply.0, /*index=5*/pred[64,2048]{1,0} %compare.2, f32[64,8192]{1,0} %maximum.2, pred[64,8192]{1,0} %compare.3), metadata={op_name="tuple.9"}
  }
  
  
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.257 GB, invar_size=0.251 GB, outvar_size=0.006 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
-----------------------------------------------------------------------backward-------------------------------------------------------------------  
  
  
  input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
  output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
  donated_invars (True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False)
  hlo module: HloModule stage_0_1-stage_0_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[64,2048]{1,0},pred[64,2048]{1,0},f32[64,8192]{1,0},pred[64,8192]{1,0},f32[8192,2048]{1,0},f32[64,2048]{1,0},f32[2048,8192]{1,0},pred[64,2048]{1,0},f32[64,8192]{1,0},pred[64,8192]{1,0},f32[64,2048]{1,0},f32[8192,2048]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0})}
  
  %region_0.300.stage_0_1_acc_grad_1 (Arg_0.301: f32[], Arg_1.302: f32[]) -> f32[] {
    %Arg_0.301 = f32[] parameter(0)
    %Arg_1.302 = f32[] parameter(1)
    ROOT %add.303 = f32[] add(f32[] %Arg_0.301, f32[] %Arg_1.302), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %add (x: f32[], y: f32[]) -> f32[] {
    %x = f32[] parameter(0)
    %y = f32[] parameter(1)
    ROOT %add = f32[] add(f32[] %x, f32[] %y)
  }
  
  %region_1.304.stage_0_1_acc_grad_1 (Arg_0.305: f32[], Arg_1.306: f32[]) -> f32[] {
    %Arg_0.305 = f32[] parameter(0)
    %Arg_1.306 = f32[] parameter(1)
    ROOT %add.307 = f32[] add(f32[] %Arg_0.305, f32[] %Arg_1.306), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %add.1 (x.1: f32[], y.1: f32[]) -> f32[] {
    %x.1 = f32[] parameter(0)
    %y.1 = f32[] parameter(1)
    ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)
  }
  
  %region_2.343.stage_0_1_acc_grad_1 (Arg_0.344: f32[], Arg_1.345: f32[]) -> f32[] {
    %Arg_0.344 = f32[] parameter(0)
    %Arg_1.345 = f32[] parameter(1)
    ROOT %add.346 = f32[] add(f32[] %Arg_0.344, f32[] %Arg_1.345), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %add.2 (x.2: f32[], y.2: f32[]) -> f32[] {
    %x.2 = f32[] parameter(0)
    %y.2 = f32[] parameter(1)
    ROOT %add.2 = f32[] add(f32[] %x.2, f32[] %y.2)
  }
  
  %region_3.347.stage_0_1_acc_grad_1 (Arg_0.348: f32[], Arg_1.349: f32[]) -> f32[] {
    %Arg_0.348 = f32[] parameter(0)
    %Arg_1.349 = f32[] parameter(1)
    ROOT %add.350 = f32[] add(f32[] %Arg_0.348, f32[] %Arg_1.349), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %add.3 (x.3: f32[], y.3: f32[]) -> f32[] {
    %x.3 = f32[] parameter(0)
    %y.3 = f32[] parameter(1)
    ROOT %add.3 = f32[] add(f32[] %x.3, f32[] %y.3)
  }
  
  ENTRY %main.1125-stage_0_1_acc_grad_1_spmd (param: f32[2048], param.3: f32[8192,2048], param.5: f32[8192], param.8: f32[2048,8192], param.10: f32[2048], param.13: f32[8192,2048], param.15: f32[8192], param.18: f32[2048,8192], param.2: f32[64,2048], param.1: pred[64,2048], param.4: f32[64,8192], param.6: pred[64,8192], param.7: f32[8192,2048], param.9: f32[64,2048], param.12: f32[2048,8192], param.11: pred[64,2048], param.14: f32[64,8192], param.16: pred[64,8192], param.19: f32[64,2048], param.17: f32[8192,2048]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192], f32[2048], /*index=5*/f32[8192,2048], f32[8192], f32[2048,8192]) {
    %param = f32[2048]{0} parameter(0), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.1 = pred[64,2048]{1,0} parameter(9), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.2 = f32[64,2048]{1,0} parameter(8), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %constant = f32[] constant(3.81469727e-06)
    %broadcast.3 = f32[64,2048]{1,0} broadcast(f32[] %constant), dimensions={}
    %multiply.0 = f32[64,2048]{1,0} multiply(f32[64,2048]{1,0} %param.2, f32[64,2048]{1,0} %broadcast.3), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %constant.3 = f32[] constant(0)
    %broadcast.4 = f32[64,2048]{1,0} broadcast(f32[] %constant.3), dimensions={}
    %select.0 = f32[64,2048]{1,0} select(pred[64,2048]{1,0} %param.1, f32[64,2048]{1,0} %multiply.0, f32[64,2048]{1,0} %broadcast.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %reduce = f32[2048]{0} reduce(f32[64,2048]{1,0} %select.0, f32[] %constant.3), dimensions={0}, to_apply=%region_0.300.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.4 = f32[2048]{0} add(f32[2048]{0} %param, f32[2048]{0} %reduce), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %all-reduce = f32[2048]{0} all-reduce(f32[2048]{0} %add.4), channel_id=1, replica_groups={{0}}, to_apply=%region_0.300.stage_0_1_acc_grad_1, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %param.3 = f32[8192,2048]{1,0} parameter(1), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.4 = f32[64,8192]{1,0} parameter(10), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot = f32[8192,2048]{1,0} dot(f32[64,8192]{1,0} %param.4, f32[64,2048]{1,0} %select.0), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.5 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %param.3, f32[8192,2048]{1,0} %dot), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %all-reduce.1 = f32[8192,2048]{1,0} all-reduce(f32[8192,2048]{1,0} %add.5), channel_id=2, replica_groups={{0}}, to_apply=%add, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.5 = f32[8192]{0} parameter(2), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.6 = pred[64,8192]{1,0} parameter(11), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.7 = f32[8192,2048]{1,0} parameter(12), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.1 = f32[64,8192]{1,0} dot(f32[64,2048]{1,0} %select.0, f32[8192,2048]{1,0} %param.7), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %broadcast.5 = f32[64,8192]{1,0} broadcast(f32[] %constant.3), dimensions={}
    %select.1 = f32[64,8192]{1,0} select(pred[64,8192]{1,0} %param.6, f32[64,8192]{1,0} %dot.1, f32[64,8192]{1,0} %broadcast.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    %reduce.1 = f32[8192]{0} reduce(f32[64,8192]{1,0} %select.1, f32[] %constant.3), dimensions={0}, to_apply=%region_1.304.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.6 = f32[8192]{0} add(f32[8192]{0} %param.5, f32[8192]{0} %reduce.1), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %all-reduce.2 = f32[8192]{0} all-reduce(f32[8192]{0} %add.6), channel_id=3, replica_groups={{0}}, to_apply=%region_1.304.stage_0_1_acc_grad_1, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %param.8 = f32[2048,8192]{1,0} parameter(3), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.9 = f32[64,2048]{1,0} parameter(13), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.2 = f32[2048,8192]{1,0} dot(f32[64,2048]{1,0} %param.9, f32[64,8192]{1,0} %select.1), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.7 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %param.8, f32[2048,8192]{1,0} %dot.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %all-reduce.3 = f32[2048,8192]{1,0} all-reduce(f32[2048,8192]{1,0} %add.7), channel_id=4, replica_groups={{0}}, to_apply=%add.1, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.10 = f32[2048]{0} parameter(4), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.11 = pred[64,2048]{1,0} parameter(15), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.12 = f32[2048,8192]{1,0} parameter(14), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.3 = f32[64,2048]{1,0} dot(f32[64,8192]{1,0} %select.1, f32[2048,8192]{1,0} %param.12), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %select.2 = f32[64,2048]{1,0} select(pred[64,2048]{1,0} %param.11, f32[64,2048]{1,0} %dot.3, f32[64,2048]{1,0} %broadcast.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %reduce.2 = f32[2048]{0} reduce(f32[64,2048]{1,0} %select.2, f32[] %constant.3), dimensions={0}, to_apply=%region_2.343.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.8 = f32[2048]{0} add(f32[2048]{0} %param.10, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %all-reduce.4 = f32[2048]{0} all-reduce(f32[2048]{0} %add.8), channel_id=5, replica_groups={{0}}, to_apply=%region_2.343.stage_0_1_acc_grad_1, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %param.13 = f32[8192,2048]{1,0} parameter(5), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.14 = f32[64,8192]{1,0} parameter(16), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.4 = f32[8192,2048]{1,0} dot(f32[64,8192]{1,0} %param.14, f32[64,2048]{1,0} %select.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.9 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %param.13, f32[8192,2048]{1,0} %dot.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %all-reduce.5 = f32[8192,2048]{1,0} all-reduce(f32[8192,2048]{1,0} %add.9), channel_id=6, replica_groups={{0}}, to_apply=%add.2, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.15 = f32[8192]{0} parameter(6), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.16 = pred[64,8192]{1,0} parameter(17), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.17 = f32[8192,2048]{1,0} parameter(19), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.5 = f32[64,8192]{1,0} dot(f32[64,2048]{1,0} %select.2, f32[8192,2048]{1,0} %param.17), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %select.3 = f32[64,8192]{1,0} select(pred[64,8192]{1,0} %param.16, f32[64,8192]{1,0} %dot.5, f32[64,8192]{1,0} %broadcast.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %reduce.3 = f32[8192]{0} reduce(f32[64,8192]{1,0} %select.3, f32[] %constant.3), dimensions={0}, to_apply=%region_3.347.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.10 = f32[8192]{0} add(f32[8192]{0} %param.15, f32[8192]{0} %reduce.3), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %all-reduce.6 = f32[8192]{0} all-reduce(f32[8192]{0} %add.10), channel_id=7, replica_groups={{0}}, to_apply=%region_3.347.stage_0_1_acc_grad_1, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %param.18 = f32[2048,8192]{1,0} parameter(7), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.19 = f32[64,2048]{1,0} parameter(18), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.6 = f32[2048,8192]{1,0} dot(f32[64,2048]{1,0} %param.19, f32[64,8192]{1,0} %select.3), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.11 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %param.18, f32[2048,8192]{1,0} %dot.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %all-reduce.7 = f32[2048,8192]{1,0} all-reduce(f32[2048,8192]{1,0} %add.11), channel_id=8, replica_groups={{0}}, to_apply=%add.3, metadata={op_name="grad_acc_skippable_all_reduce" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    ROOT %tuple = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %all-reduce, f32[8192,2048]{1,0} %all-reduce.1, f32[8192]{0} %all-reduce.2, f32[2048,8192]{1,0} %all-reduce.3, f32[2048]{0} %all-reduce.4, /*index=5*/f32[8192,2048]{1,0} %all-reduce.5, f32[8192]{0} %all-reduce.6, f32[2048,8192]{1,0} %all-reduce.7), metadata={op_name="tuple.9"}
  }
  
  
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=0.694 GB, invar_size=0.444 GB, outvar_size=0.250 GB, temp_buffer_size=0.250 GB, available_memory=20.882 GB)

---------------------------------------------------------------------------------------------------------------------------------------------------------------



  input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
  output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
  donated_invars (False, False, False, False, False, False, False, False, False, False)
  hlo module: HloModule stage_0_1-stage_0_1_acc_grad_0, entry_computation_layout={(f32[2048,4096]{1,0},f32[4096]{0},f32[4096,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0},f32[2048,4096]{1,0},f32[4096]{0},f32[4096,2048]{1,0},f32[2048]{0},f32[64,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,4096]{1,0}, pred[128,4096]{1,0}, f32[64,2048]{1,0}, /*index=5*/pred[64,2048]{1,0}, f32[128,4096]{1,0}, pred[128,4096]{1,0})}
  
  %add (x: f32[], y: f32[]) -> f32[] {
    %x = f32[] parameter(0)
    %y = f32[] parameter(1)
    ROOT %add = f32[] add(f32[] %x, f32[] %y)
  }
  
  %add.1 (x.1: f32[], y.1: f32[]) -> f32[] {
    %x.1 = f32[] parameter(0)
    %y.1 = f32[] parameter(1)
    ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)
  }
  
  ENTRY %main.1125-stage_0_1_acc_grad_0_spmd (param.1: f32[2048,4096], param.2: f32[4096], param.3: f32[4096,2048], param.4: f32[2048], param: f32[128,2048], param.5: f32[2048,4096], param.6: f32[4096], param.7: f32[4096,2048], param.8: f32[2048], param.9: f32[64,2048]) -> (f32[128,2048], pred[128,2048], f32[128,4096], pred[128,4096], f32[64,2048], /*index=5*/pred[64,2048], f32[128,4096], pred[128,4096]) {
    %param = f32[128,2048]{1,0} parameter(4), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %param.1 = f32[2048,4096]{1,0} parameter(0), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot = f32[128,4096]{1,0} dot(f32[128,2048]{1,0} %param, f32[2048,4096]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.2 = f32[4096]{0} parameter(1), sharding={devices=[2]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.0 = f32[128,4096]{1,0} broadcast(f32[4096]{0} %param.2), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.2 = f32[128,4096]{1,0} add(f32[128,4096]{1,0} %dot, f32[128,4096]{1,0} %broadcast.0), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %constant = f32[] constant(0)
    %broadcast.7 = f32[128,4096]{1,0} broadcast(f32[] %constant), dimensions={}
    %maximum.0 = f32[128,4096]{1,0} maximum(f32[128,4096]{1,0} %add.2, f32[128,4096]{1,0} %broadcast.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %param.3 = f32[4096,2048]{1,0} parameter(2), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.1 = f32[128,2048]{1,0} dot(f32[128,4096]{1,0} %maximum.0, f32[4096,2048]{1,0} %param.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %all-reduce = f32[128,2048]{1,0} all-reduce(f32[128,2048]{1,0} %dot.1), channel_id=1, replica_groups={{0}}, to_apply=%add, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.4 = f32[2048]{0} parameter(3), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.2 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %param.4), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.3 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %all-reduce, f32[128,2048]{1,0} %broadcast.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %broadcast.3 = f32[128,2048]{1,0} broadcast(f32[] %constant), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %maximum.1 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.3, f32[128,2048]{1,0} %broadcast.3), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %compare.0 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.3, f32[128,2048]{1,0} %broadcast.3), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %compare.1 = pred[128,4096]{1,0} compare(f32[128,4096]{1,0} %add.2, f32[128,4096]{1,0} %broadcast.7), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %param.5 = f32[2048,4096]{1,0} parameter(5), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.2 = f32[128,4096]{1,0} dot(f32[128,2048]{1,0} %maximum.1, f32[2048,4096]{1,0} %param.5), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.6 = f32[4096]{0} parameter(6), sharding={devices=[2]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.4 = f32[128,4096]{1,0} broadcast(f32[4096]{0} %param.6), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.4 = f32[128,4096]{1,0} add(f32[128,4096]{1,0} %dot.2, f32[128,4096]{1,0} %broadcast.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %maximum.2 = f32[128,4096]{1,0} maximum(f32[128,4096]{1,0} %add.4, f32[128,4096]{1,0} %broadcast.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    %param.7 = f32[4096,2048]{1,0} parameter(7), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %dot.3 = f32[128,2048]{1,0} dot(f32[128,4096]{1,0} %maximum.2, f32[4096,2048]{1,0} %param.7), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %all-reduce.1 = f32[128,2048]{1,0} all-reduce(f32[128,2048]{1,0} %dot.3), channel_id=2, replica_groups={{0}}, to_apply=%add.1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %constant.8 = s32[2]{0} constant({0, 64}), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %partition-id = u32[] partition-id()
    %dynamic-slice.2 = s32[1]{0} dynamic-slice(s32[2]{0} %constant.8, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %reshape.1 = s32[] reshape(s32[1]{0} %dynamic-slice.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %constant.1 = s32[] constant(0), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %dynamic-slice.3 = f32[64,2048]{1,0} dynamic-slice(f32[128,2048]{1,0} %all-reduce.1, s32[] %reshape.1, s32[] %constant.1), dynamic_slice_sizes={64,2048}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %param.8 = f32[2048]{0} parameter(8), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %broadcast.5 = f32[64,2048]{1,0} broadcast(f32[2048]{0} %param.8), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.5 = f32[64,2048]{1,0} add(f32[64,2048]{1,0} %dynamic-slice.3, f32[64,2048]{1,0} %broadcast.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %broadcast.8 = f32[64,2048]{1,0} broadcast(f32[] %constant), dimensions={}
    %maximum.3 = f32[64,2048]{1,0} maximum(f32[64,2048]{1,0} %add.5, f32[64,2048]{1,0} %broadcast.8), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %param.9 = f32[64,2048]{1,0} parameter(9), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_0$start"}
    %subtract.0 = f32[64,2048]{1,0} subtract(f32[64,2048]{1,0} %maximum.3, f32[64,2048]{1,0} %param.9), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %constant.12 = f32[] constant(2)
    %broadcast.9 = f32[64,2048]{1,0} broadcast(f32[] %constant.12), dimensions={}
    %multiply.0 = f32[64,2048]{1,0} multiply(f32[64,2048]{1,0} %subtract.0, f32[64,2048]{1,0} %broadcast.9), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %compare.2 = pred[64,2048]{1,0} compare(f32[64,2048]{1,0} %add.5, f32[64,2048]{1,0} %broadcast.8), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %compare.3 = pred[128,4096]{1,0} compare(f32[128,4096]{1,0} %add.4, f32[128,4096]{1,0} %broadcast.7), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    ROOT %tuple = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,4096]{1,0}, pred[128,4096]{1,0}, f32[64,2048]{1,0}, /*index=5*/pred[64,2048]{1,0}, f32[128,4096]{1,0}, pred[128,4096]{1,0}) tuple(f32[128,2048]{1,0} %maximum.1, pred[128,2048]{1,0} %compare.0, f32[128,4096]{1,0} %maximum.0, pred[128,4096]{1,0} %compare.1, f32[64,2048]{1,0} %multiply.0, /*index=5*/pred[64,2048]{1,0} %compare.2, f32[128,4096]{1,0} %maximum.2, pred[128,4096]{1,0} %compare.3), metadata={op_name="tuple.9"}
  }
  
  
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.134 GB, invar_size=0.127 GB, outvar_size=0.007 GB, temp_buffer_size=0.001 GB, available_memory=20.882 GB)
  input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
  output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
  donated_invars (True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False)
  hlo module: HloModule stage_0_1-stage_0_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[4096,2048]{1,0},f32[4096]{0},f32[2048,4096]{1,0},f32[2048]{0},f32[4096,2048]{1,0},f32[4096]{0},f32[2048,4096]{1,0},f32[64,2048]{1,0},pred[64,2048]{1,0},f32[128,4096]{1,0},pred[128,4096]{1,0},f32[4096,2048]{1,0},f32[128,2048]{1,0},f32[2048,4096]{1,0},pred[128,2048]{1,0},f32[128,4096]{1,0},pred[128,4096]{1,0},f32[128,2048]{1,0},f32[4096,2048]{1,0})->(f32[2048]{0}, f32[4096,2048]{1,0}, f32[4096]{0}, f32[2048,4096]{1,0}, f32[2048]{0}, /*index=5*/f32[4096,2048]{1,0}, f32[4096]{0}, f32[2048,4096]{1,0})}
  
  %region_0.300.stage_0_1_acc_grad_1 (Arg_0.301: f32[], Arg_1.302: f32[]) -> f32[] {
    %Arg_0.301 = f32[] parameter(0)
    %Arg_1.302 = f32[] parameter(1)
    ROOT %add.303 = f32[] add(f32[] %Arg_0.301, f32[] %Arg_1.302), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %region_1.304.stage_0_1_acc_grad_1 (Arg_0.305: f32[], Arg_1.306: f32[]) -> f32[] {
    %Arg_0.305 = f32[] parameter(0)
    %Arg_1.306 = f32[] parameter(1)
    ROOT %add.307 = f32[] add(f32[] %Arg_0.305, f32[] %Arg_1.306), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %add (x: f32[], y: f32[]) -> f32[] {
    %x = f32[] parameter(0)
    %y = f32[] parameter(1)
    ROOT %add = f32[] add(f32[] %x, f32[] %y)
  }
  
  %region_2.343.stage_0_1_acc_grad_1 (Arg_0.344: f32[], Arg_1.345: f32[]) -> f32[] {
    %Arg_0.344 = f32[] parameter(0)
    %Arg_1.345 = f32[] parameter(1)
    ROOT %add.346 = f32[] add(f32[] %Arg_0.344, f32[] %Arg_1.345), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  %region_3.347.stage_0_1_acc_grad_1 (Arg_0.348: f32[], Arg_1.349: f32[]) -> f32[] {
    %Arg_0.348 = f32[] parameter(0)
    %Arg_1.349 = f32[] parameter(1)
    ROOT %add.350 = f32[] add(f32[] %Arg_0.348, f32[] %Arg_1.349), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  }
  
  ENTRY %main.1125-stage_0_1_acc_grad_1_spmd (param: f32[2048], param.3: f32[4096,2048], param.5: f32[4096], param.8: f32[2048,4096], param.10: f32[2048], param.13: f32[4096,2048], param.15: f32[4096], param.18: f32[2048,4096], param.2: f32[64,2048], param.1: pred[64,2048], param.4: f32[128,4096], param.6: pred[128,4096], param.7: f32[4096,2048], param.9: f32[128,2048], param.12: f32[2048,4096], param.11: pred[128,2048], param.14: f32[128,4096], param.16: pred[128,4096], param.19: f32[128,2048], param.17: f32[4096,2048]) -> (f32[2048], f32[4096,2048], f32[4096], f32[2048,4096], f32[2048], /*index=5*/f32[4096,2048], f32[4096], f32[2048,4096]) {
    %param = f32[2048]{0} parameter(0), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.1 = pred[64,2048]{1,0} parameter(9), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.2 = f32[64,2048]{1,0} parameter(8), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %constant = f32[] constant(3.81469727e-06)
    %broadcast.3 = f32[64,2048]{1,0} broadcast(f32[] %constant), dimensions={}
    %multiply.0 = f32[64,2048]{1,0} multiply(f32[64,2048]{1,0} %param.2, f32[64,2048]{1,0} %broadcast.3), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
    %constant.3 = f32[] constant(0)
    %broadcast.4 = f32[64,2048]{1,0} broadcast(f32[] %constant.3), dimensions={}
    %select.0 = f32[64,2048]{1,0} select(pred[64,2048]{1,0} %param.1, f32[64,2048]{1,0} %multiply.0, f32[64,2048]{1,0} %broadcast.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
    %reshape.2 = f32[1,64,2048]{2,1,0} reshape(f32[64,2048]{1,0} %select.0)
    %all-gather = f32[2,64,2048]{2,1,0} all-gather(f32[1,64,2048]{2,1,0} %reshape.2), channel_id=1, replica_groups={{0,1}}, dimensions={0}, use_global_device_ids=true
    %transpose = f32[2,64,2048]{2,1,0} transpose(f32[2,64,2048]{2,1,0} %all-gather), dimensions={0,1,2}
    %reshape.3 = f32[128,2048]{1,0} reshape(f32[2,64,2048]{2,1,0} %transpose)
    %reshape.4 = f32[128,2048]{1,0} reshape(f32[128,2048]{1,0} %reshape.3)
    %reduce = f32[2048]{0} reduce(f32[128,2048]{1,0} %reshape.4, f32[] %constant.3), dimensions={0}, to_apply=%region_0.300.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.1 = f32[2048]{0} add(f32[2048]{0} %param, f32[2048]{0} %reduce), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %param.3 = f32[4096,2048]{1,0} parameter(1), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.4 = f32[128,4096]{1,0} parameter(10), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot = f32[4096,2048]{1,0} dot(f32[128,4096]{1,0} %param.4, f32[128,2048]{1,0} %reshape.4), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.2 = f32[4096,2048]{1,0} add(f32[4096,2048]{1,0} %param.3, f32[4096,2048]{1,0} %dot), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %param.5 = f32[4096]{0} parameter(2), sharding={devices=[2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.6 = pred[128,4096]{1,0} parameter(11), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.7 = f32[4096,2048]{1,0} parameter(12), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.1 = f32[128,4096]{1,0} dot(f32[128,2048]{1,0} %reshape.4, f32[4096,2048]{1,0} %param.7), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %broadcast.5 = f32[128,4096]{1,0} broadcast(f32[] %constant.3), dimensions={}
    %select.1 = f32[128,4096]{1,0} select(pred[128,4096]{1,0} %param.6, f32[128,4096]{1,0} %dot.1, f32[128,4096]{1,0} %broadcast.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
    %reduce.1 = f32[4096]{0} reduce(f32[128,4096]{1,0} %select.1, f32[] %constant.3), dimensions={0}, to_apply=%region_1.304.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.3 = f32[4096]{0} add(f32[4096]{0} %param.5, f32[4096]{0} %reduce.1), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %param.8 = f32[2048,4096]{1,0} parameter(3), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.9 = f32[128,2048]{1,0} parameter(13), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.2 = f32[2048,4096]{1,0} dot(f32[128,2048]{1,0} %param.9, f32[128,4096]{1,0} %select.1), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.4 = f32[2048,4096]{1,0} add(f32[2048,4096]{1,0} %param.8, f32[2048,4096]{1,0} %dot.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
    %param.10 = f32[2048]{0} parameter(4), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.11 = pred[128,2048]{1,0} parameter(15), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.12 = f32[2048,4096]{1,0} parameter(14), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.3 = f32[128,2048]{1,0} dot(f32[128,4096]{1,0} %select.1, f32[2048,4096]{1,0} %param.12), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %all-reduce = f32[128,2048]{1,0} all-reduce(f32[128,2048]{1,0} %dot.3), channel_id=2, replica_groups={{0}}, to_apply=%add, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %broadcast.1 = f32[128,2048]{1,0} broadcast(f32[] %constant.3), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %select.2 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %param.11, f32[128,2048]{1,0} %all-reduce, f32[128,2048]{1,0} %broadcast.1), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
    %reduce.2 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.2, f32[] %constant.3), dimensions={0}, to_apply=%region_2.343.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.5 = f32[2048]{0} add(f32[2048]{0} %param.10, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %param.13 = f32[4096,2048]{1,0} parameter(5), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.14 = f32[128,4096]{1,0} parameter(16), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.4 = f32[4096,2048]{1,0} dot(f32[128,4096]{1,0} %param.14, f32[128,2048]{1,0} %select.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.6 = f32[4096,2048]{1,0} add(f32[4096,2048]{1,0} %param.13, f32[4096,2048]{1,0} %dot.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %param.15 = f32[4096]{0} parameter(6), sharding={devices=[2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.16 = pred[128,4096]{1,0} parameter(17), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.17 = f32[4096,2048]{1,0} parameter(19), sharding={devices=[2,1]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.5 = f32[128,4096]{1,0} dot(f32[128,2048]{1,0} %select.2, f32[4096,2048]{1,0} %param.17), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %select.3 = f32[128,4096]{1,0} select(pred[128,4096]{1,0} %param.16, f32[128,4096]{1,0} %dot.5, f32[128,4096]{1,0} %broadcast.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
    %reduce.3 = f32[4096]{0} reduce(f32[128,4096]{1,0} %select.3, f32[] %constant.3), dimensions={0}, to_apply=%region_3.347.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
    %add.7 = f32[4096]{0} add(f32[4096]{0} %param.15, f32[4096]{0} %reduce.3), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    %param.18 = f32[2048,4096]{1,0} parameter(7), sharding={devices=[1,2]0,1}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %param.19 = f32[128,2048]{1,0} parameter(18), sharding={replicated}, metadata={op_name="stage_0_1_acc_grad_1$start"}
    %dot.6 = f32[2048,4096]{1,0} dot(f32[128,2048]{1,0} %param.19, f32[128,4096]{1,0} %select.3), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
    %add.8 = f32[2048,4096]{1,0} add(f32[2048,4096]{1,0} %param.18, f32[2048,4096]{1,0} %dot.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
    ROOT %tuple = (f32[2048]{0}, f32[4096,2048]{1,0}, f32[4096]{0}, f32[2048,4096]{1,0}, f32[2048]{0}, /*index=5*/f32[4096,2048]{1,0}, f32[4096]{0}, f32[2048,4096]{1,0}) tuple(f32[2048]{0} %add.1, f32[4096,2048]{1,0} %add.2, f32[4096]{0} %add.3, f32[2048,4096]{1,0} %add.4, f32[2048]{0} %add.5, /*index=5*/f32[4096,2048]{1,0} %add.6, f32[4096]{0} %add.7, f32[2048,4096]{1,0} %add.8), metadata={op_name="tuple.9"}
  }
  
  
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.230 GB, invar_size=0.226 GB, outvar_size=0.125 GB, temp_buffer_size=0.004 GB, available_memory=20.882 GB)
Profiling for submesh 1 (2, 1) takes 13.18 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
selected_indices:  [[0], [3]]
selected_indices:  [[0], [3]]
selected_indices:  [[0, 1], [2, 3]]
selected_indices:  [[0, 1], [2, 3]]
selected_indices:  [[1], [2]]
selected_indices:  [[1], [2]]
- Compile all stages
- Profile all stages
[2m[36m(ProfileWorker pid=1524907)[0m input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524907)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524907)[0m donated_invars (False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524907)[0m hlo module: HloModule stage_0_0-stage_0_0_acc_grad_0, entry_computation_layout={(f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524907)[0m 
[2m[36m(ProfileWorker pid=1524907)[0m ENTRY %main.597-stage_0_0_acc_grad_0 (param_0: f32[2048,8192], param_1: f32[8192], param_2: f32[8192,2048], param_3: f32[2048], param_4: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524907)[0m   %param_0 = f32[2048,8192]{1,0} parameter(0), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524907)[0m   %param_1 = f32[8192]{0} parameter(1), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524907)[0m   %param_2 = f32[8192,2048]{1,0} parameter(2), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524907)[0m   %param_3 = f32[2048]{0} parameter(3), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524907)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524907)[0m   %tuple.10 = (f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) tuple(f32[2048,8192]{1,0} %param_0, f32[8192]{0} %param_1, f32[8192,2048]{1,0} %param_2, f32[2048]{0} %param_3, f32[128,2048]{1,0} %param_4)
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.47 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=4
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.43 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=0
[2m[36m(ProfileWorker pid=1524907)[0m   %dot.5 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.47, f32[2048,8192]{1,0} %get-tuple-element.43), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.44 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=1
[2m[36m(ProfileWorker pid=1524907)[0m   %reshape.4 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.44), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.44 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.4), dimensions={0,1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %reshape.5 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.44), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.45 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.5), dimensions={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %add.24 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.5, f32[128,8192]{1,0} %broadcast.45), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %constant.38 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.46 = f32[128,8192]{1,0} broadcast(f32[] %constant.38), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524907)[0m   %maximum.2 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.24, f32[128,8192]{1,0} %broadcast.46), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.45 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=2
[2m[36m(ProfileWorker pid=1524907)[0m   %dot.6 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.2, f32[8192,2048]{1,0} %get-tuple-element.45), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.46 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=3
[2m[36m(ProfileWorker pid=1524907)[0m   %reshape.6 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.46), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.47 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.6), dimensions={0,1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %reshape.7 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.47), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.48 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.7), dimensions={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %add.25 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.6, f32[128,2048]{1,0} %broadcast.48), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524907)[0m   %constant.39 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524907)[0m   %broadcast.49 = f32[128,2048]{1,0} broadcast(f32[] %constant.39), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524907)[0m   %maximum.3 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.25, f32[128,2048]{1,0} %broadcast.49), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524907)[0m   %compare.3 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.25, f32[128,2048]{1,0} %broadcast.49), direction=GT, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524907)[0m   %compare.4 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.24, f32[128,8192]{1,0} %broadcast.46), direction=GT, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524907)[0m   %tuple.11 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %maximum.3, pred[128,2048]{1,0} %compare.3, f32[128,8192]{1,0} %maximum.2, pred[128,8192]{1,0} %compare.4)
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.5 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=0
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.6 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=1
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.7 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=2
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.8 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=3
[2m[36m(ProfileWorker pid=1524907)[0m   %tuple.12 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.5, pred[128,2048]{1,0} %get-tuple-element.6, f32[128,8192]{1,0} %get-tuple-element.7, pred[128,8192]{1,0} %get-tuple-element.8)
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.88 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=0
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.89 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=1
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.90 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=2
[2m[36m(ProfileWorker pid=1524907)[0m   %get-tuple-element.91 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=3
[2m[36m(ProfileWorker pid=1524907)[0m   ROOT %tuple.13 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.88, pred[128,2048]{1,0} %get-tuple-element.89, f32[128,8192]{1,0} %get-tuple-element.90, pred[128,8192]{1,0} %get-tuple-element.91), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524907)[0m }
[2m[36m(ProfileWorker pid=1524907)[0m 
[2m[36m(ProfileWorker pid=1524907)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_0-stage_0_0_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[128,2048]{1,0},f32[8192,2048]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.166.stage_0_0_acc_grad_1 (Arg_0.167: f32[], Arg_1.168: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.167 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.168 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.169 = f32[] add(f32[] %Arg_0.167, f32[] %Arg_1.168), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.162.stage_0_0_acc_grad_1 (Arg_0.163: f32[], Arg_1.164: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.163 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.164 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.165 = f32[] add(f32[] %Arg_0.163, f32[] %Arg_1.164), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.597-stage_0_0_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[128,2048], param_5: pred[128,2048], param_6: f32[128,8192], param_7: pred[128,8192], param_8: f32[128,2048], param_9: f32[8192,2048]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = pred[128,2048]{1,0} parameter(5), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[128,8192]{1,0} parameter(6), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = pred[128,8192]{1,0} parameter(7), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[128,2048]{1,0} parameter(8), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[8192,2048]{1,0} parameter(9), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/pred[128,2048]{1,0} %param_5, f32[128,8192]{1,0} %param_6, pred[128,8192]{1,0} %param_7, f32[128,2048]{1,0} %param_8, f32[8192,2048]{1,0} %param_9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.152 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.157 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.156 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.40 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.50 = f32[128,2048]{1,0} broadcast(f32[] %constant.40), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.3 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.157, f32[128,2048]{1,0} %get-tuple-element.156, f32[128,2048]{1,0} %broadcast.50), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.41 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.2 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.3, f32[] %constant.41), dimensions={0}, to_apply=%region_0.162.stage_0_0_acc_grad_1, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.26 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.152, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.153 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.158 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.7 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[128,8192]{1,0} %get-tuple-element.158), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.2 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.7), dimensions={1,0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.27 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.153, f32[8192,2048]{0,1} %transpose.2), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.154 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.159 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.161 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.8 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[8192,2048]{1,0} %get-tuple-element.161), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.42 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.51 = f32[128,8192]{1,0} broadcast(f32[] %constant.42), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.4 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.159, f32[128,8192]{1,0} %dot.8, f32[128,8192]{1,0} %broadcast.51), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.3 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.4, f32[] %constant.41), dimensions={0}, to_apply=%region_1.166.stage_0_0_acc_grad_1, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.28 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.154, f32[8192]{0} %reduce.3), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.155 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.160 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.9 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[128,2048]{1,0} %get-tuple-element.160), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.3 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.9), dimensions={1,0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.29 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.155, f32[2048,8192]{0,1} %transpose.3), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.17 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %add.26, f32[8192,2048]{1,0} %add.27, f32[8192]{0} %add.28, f32[2048,8192]{1,0} %add.29)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.23 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.24 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.25 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.26 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.18 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.23, f32[8192,2048]{1,0} %get-tuple-element.24, f32[8192]{0} %get-tuple-element.25, f32[2048,8192]{1,0} %get-tuple-element.26)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.218 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.219 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.220 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.221 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.19 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.218, f32[8192,2048]{1,0} %get-tuple-element.219, f32[8192]{0} %get-tuple-element.220, f32[2048,8192]{1,0} %get-tuple-element.221), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.202 GB, invar_size=0.195 GB, outvar_size=0.125 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_0-stage_0_0_acc_grad_0, entry_computation_layout={(f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.597-stage_0_0_acc_grad_0 (param_0: f32[2048,8192], param_1: f32[8192], param_2: f32[8192,2048], param_3: f32[2048], param_4: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048,8192]{1,0} parameter(0), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192]{0} parameter(1), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192,2048]{1,0} parameter(2), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048]{0} parameter(3), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_0_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.10 = (f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) tuple(f32[2048,8192]{1,0} %param_0, f32[8192]{0} %param_1, f32[8192,2048]{1,0} %param_2, f32[2048]{0} %param_3, f32[128,2048]{1,0} %param_4)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.47 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.43 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.5 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.47, f32[2048,8192]{1,0} %get-tuple-element.43), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.44 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.4 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.44), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.44 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.4), dimensions={0,1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.5 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.44), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.45 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.5), dimensions={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.24 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.5, f32[128,8192]{1,0} %broadcast.45), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.38 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.46 = f32[128,8192]{1,0} broadcast(f32[] %constant.38), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.2 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.24, f32[128,8192]{1,0} %broadcast.46), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.45 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.6 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.2, f32[8192,2048]{1,0} %get-tuple-element.45), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.46 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.10), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.6 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.46), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.47 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.6), dimensions={0,1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.7 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.47), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.48 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.7), dimensions={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.25 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.6, f32[128,2048]{1,0} %broadcast.48), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.39 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.49 = f32[128,2048]{1,0} broadcast(f32[] %constant.39), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.3 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.25, f32[128,2048]{1,0} %broadcast.49), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.3 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.25, f32[128,2048]{1,0} %broadcast.49), direction=GT, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.4 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.24, f32[128,8192]{1,0} %broadcast.46), direction=GT, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_0)/jit(stage_0_0_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.11 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %maximum.3, pred[128,2048]{1,0} %compare.3, f32[128,8192]{1,0} %maximum.2, pred[128,8192]{1,0} %compare.4)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.5 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.6 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.7 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.8 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.12 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.5, pred[128,2048]{1,0} %get-tuple-element.6, f32[128,8192]{1,0} %get-tuple-element.7, pred[128,8192]{1,0} %get-tuple-element.8)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.88 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.89 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.90 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.91 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.13 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.88, pred[128,2048]{1,0} %get-tuple-element.89, f32[128,8192]{1,0} %get-tuple-element.90, pred[128,8192]{1,0} %get-tuple-element.91), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.132 GB, invar_size=0.126 GB, outvar_size=0.006 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_0-stage_0_0_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[128,2048]{1,0},f32[8192,2048]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.166.stage_0_0_acc_grad_1 (Arg_0.167: f32[], Arg_1.168: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.167 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.168 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.169 = f32[] add(f32[] %Arg_0.167, f32[] %Arg_1.168), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.162.stage_0_0_acc_grad_1 (Arg_0.163: f32[], Arg_1.164: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.163 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.164 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.165 = f32[] add(f32[] %Arg_0.163, f32[] %Arg_1.164), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.597-stage_0_0_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[128,2048], param_5: pred[128,2048], param_6: f32[128,8192], param_7: pred[128,8192], param_8: f32[128,2048], param_9: f32[8192,2048]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = pred[128,2048]{1,0} parameter(5), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[128,8192]{1,0} parameter(6), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = pred[128,8192]{1,0} parameter(7), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[128,2048]{1,0} parameter(8), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[8192,2048]{1,0} parameter(9), metadata={op_name="stage_0_0_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/pred[128,2048]{1,0} %param_5, f32[128,8192]{1,0} %param_6, pred[128,8192]{1,0} %param_7, f32[128,2048]{1,0} %param_8, f32[8192,2048]{1,0} %param_9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.152 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.157 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.156 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.40 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.50 = f32[128,2048]{1,0} broadcast(f32[] %constant.40), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.3 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.157, f32[128,2048]{1,0} %get-tuple-element.156, f32[128,2048]{1,0} %broadcast.50), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.41 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.2 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.3, f32[] %constant.41), dimensions={0}, to_apply=%region_0.162.stage_0_0_acc_grad_1, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.26 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.152, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.153 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.158 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.7 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[128,8192]{1,0} %get-tuple-element.158), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.2 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.7), dimensions={1,0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.27 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.153, f32[8192,2048]{0,1} %transpose.2), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.154 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.159 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.161 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.8 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[8192,2048]{1,0} %get-tuple-element.161), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.42 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.51 = f32[128,8192]{1,0} broadcast(f32[] %constant.42), dimensions={}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.4 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.159, f32[128,8192]{1,0} %dot.8, f32[128,8192]{1,0} %broadcast.51), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.3 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.4, f32[] %constant.41), dimensions={0}, to_apply=%region_1.166.stage_0_0_acc_grad_1, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.28 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.154, f32[8192]{0} %reduce.3), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.155 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.160 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.16), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.9 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[128,2048]{1,0} %get-tuple-element.160), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.3 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.9), dimensions={1,0}, metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.29 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.155, f32[2048,8192]{0,1} %transpose.3), metadata={op_name="parallelize(stage_0_0)/jit(main)/jit(stage_0_0_acc_grad_1)/jit(stage_0_0_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.17 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %add.26, f32[8192,2048]{1,0} %add.27, f32[8192]{0} %add.28, f32[2048,8192]{1,0} %add.29)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.23 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.24 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.25 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.26 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.17), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.18 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.23, f32[8192,2048]{1,0} %get-tuple-element.24, f32[8192]{0} %get-tuple-element.25, f32[2048,8192]{1,0} %get-tuple-element.26)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.218 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.219 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.220 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.221 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.18), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.19 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.218, f32[8192,2048]{1,0} %get-tuple-element.219, f32[8192]{0} %get-tuple-element.220, f32[2048,8192]{1,0} %get-tuple-element.221), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.202 GB, invar_size=0.195 GB, outvar_size=0.125 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (False, False, False, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_1-stage_0_1_acc_grad_0, entry_computation_layout={(f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0},f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.1125-stage_0_1_acc_grad_0 (param_0: f32[2048,8192], param_1: f32[8192], param_2: f32[8192,2048], param_3: f32[2048], param_4: f32[128,2048], param_5: f32[2048,8192], param_6: f32[8192], param_7: f32[8192,2048], param_8: f32[2048], param_9: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192], f32[128,2048], /*index=5*/pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048,8192]{1,0} parameter(0), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192]{0} parameter(1), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192,2048]{1,0} parameter(2), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048]{0} parameter(3), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[2048,8192]{1,0} parameter(5), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[8192]{0} parameter(6), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = f32[8192,2048]{1,0} parameter(7), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[2048]{0} parameter(8), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[128,2048]{1,0} parameter(9), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.13 = (f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) tuple(f32[2048,8192]{1,0} %param_0, f32[8192]{0} %param_1, f32[8192,2048]{1,0} %param_2, f32[2048]{0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/f32[2048,8192]{1,0} %param_5, f32[8192]{0} %param_6, f32[8192,2048]{1,0} %param_7, f32[2048]{0} %param_8, f32[128,2048]{1,0} %param_9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.78 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.74 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.11 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.78, f32[2048,8192]{1,0} %get-tuple-element.74), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.75 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.8 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.75), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.90 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.8), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.9 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.90), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.92 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.9), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.47 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.11, f32[128,8192]{1,0} %broadcast.92), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.78 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.94 = f32[128,8192]{1,0} broadcast(f32[] %constant.78), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.4 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.47, f32[128,8192]{1,0} %broadcast.94), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.76 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.12 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.4, f32[8192,2048]{1,0} %get-tuple-element.76), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.77 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.10 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.77), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.95 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.10), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.11 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.95), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.97 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.11), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.48 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.12, f32[128,2048]{1,0} %broadcast.97), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.79 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.99 = f32[128,2048]{1,0} broadcast(f32[] %constant.79), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.5 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.48, f32[128,2048]{1,0} %broadcast.99), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.6 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.48, f32[128,2048]{1,0} %broadcast.99), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.7 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.47, f32[128,8192]{1,0} %broadcast.94), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.14 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %maximum.5, pred[128,2048]{1,0} %compare.6, f32[128,8192]{1,0} %maximum.4, pred[128,8192]{1,0} %compare.7)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.10 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.11 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.12 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.13 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.79 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.13 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.10, f32[2048,8192]{1,0} %get-tuple-element.79), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.80 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.12 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.80), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.100 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.12), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.13 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.100), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.101 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.13), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.49 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.13, f32[128,8192]{1,0} %broadcast.101), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.80 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.102 = f32[128,8192]{1,0} broadcast(f32[] %constant.80), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.6 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.49, f32[128,8192]{1,0} %broadcast.102), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.81 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.14 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.6, f32[8192,2048]{1,0} %get-tuple-element.81), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.82 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.14 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.82), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.103 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.14), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.15 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.103), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.105 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.15), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.50 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.14, f32[128,2048]{1,0} %broadcast.105), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.81 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.107 = f32[128,2048]{1,0} broadcast(f32[] %constant.81), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.7 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.50, f32[128,2048]{1,0} %broadcast.107), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.83 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %subtract.5 = f32[128,2048]{1,0} subtract(f32[128,2048]{1,0} %maximum.7, f32[128,2048]{1,0} %get-tuple-element.83), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.82 = f32[] constant(2)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.108 = f32[128,2048]{1,0} broadcast(f32[] %constant.82), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.50 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %subtract.5, f32[128,2048]{1,0} %broadcast.108), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.8 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.50, f32[128,2048]{1,0} %broadcast.107), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.9 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.49, f32[128,8192]{1,0} %broadcast.102), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.15 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %multiply.50, pred[128,2048]{1,0} %compare.8, f32[128,8192]{1,0} %maximum.6, pred[128,8192]{1,0} %compare.9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.14 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.15 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.16 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.17 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.10, pred[128,2048]{1,0} %get-tuple-element.11, f32[128,8192]{1,0} %get-tuple-element.12, pred[128,8192]{1,0} %get-tuple-element.13, f32[128,2048]{1,0} %get-tuple-element.14, /*index=5*/pred[128,2048]{1,0} %get-tuple-element.15, f32[128,8192]{1,0} %get-tuple-element.16, pred[128,8192]{1,0} %get-tuple-element.17)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.166 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.167 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.168 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.169 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.170 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.171 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.172 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.173 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.17 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.166, pred[128,2048]{1,0} %get-tuple-element.167, f32[128,8192]{1,0} %get-tuple-element.168, pred[128,8192]{1,0} %get-tuple-element.169, f32[128,2048]{1,0} %get-tuple-element.170, /*index=5*/pred[128,2048]{1,0} %get-tuple-element.171, f32[128,8192]{1,0} %get-tuple-element.172, pred[128,8192]{1,0} %get-tuple-element.173), metadata={op_name="tuple.9"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.264 GB, invar_size=0.252 GB, outvar_size=0.012 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_1-stage_0_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[8192,2048]{1,0},f32[128,2048]{1,0},f32[2048,8192]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[128,2048]{1,0},f32[8192,2048]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.304.stage_0_1_acc_grad_1 (Arg_0.305: f32[], Arg_1.306: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.305 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.306 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.307 = f32[] add(f32[] %Arg_0.305, f32[] %Arg_1.306), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.300.stage_0_1_acc_grad_1 (Arg_0.301: f32[], Arg_1.302: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.301 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.302 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.303 = f32[] add(f32[] %Arg_0.301, f32[] %Arg_1.302), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_3.347.stage_0_1_acc_grad_1 (Arg_0.348: f32[], Arg_1.349: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.348 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.349 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.350 = f32[] add(f32[] %Arg_0.348, f32[] %Arg_1.349), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_2.343.stage_0_1_acc_grad_1 (Arg_0.344: f32[], Arg_1.345: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.344 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.345 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.346 = f32[] add(f32[] %Arg_0.344, f32[] %Arg_1.345), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.1125-stage_0_1_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[2048], param_5: f32[8192,2048], param_6: f32[8192], param_7: f32[2048,8192], param_8: f32[128,2048], param_9: pred[128,2048], param_10: f32[128,8192], param_11: pred[128,8192], param_12: f32[8192,2048], param_13: f32[128,2048], param_14: f32[2048,8192], param_15: pred[128,2048], param_16: f32[128,8192], param_17: pred[128,8192], param_18: f32[128,2048], param_19: f32[8192,2048]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192], f32[2048], /*index=5*/f32[8192,2048], f32[8192], f32[2048,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[2048]{0} parameter(4), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[8192,2048]{1,0} parameter(5), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[8192]{0} parameter(6), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = f32[2048,8192]{1,0} parameter(7), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[128,2048]{1,0} parameter(8), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = pred[128,2048]{1,0} parameter(9), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_10 = f32[128,8192]{1,0} parameter(10), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_11 = pred[128,8192]{1,0} parameter(11), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_12 = f32[8192,2048]{1,0} parameter(12), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_13 = f32[128,2048]{1,0} parameter(13), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_14 = f32[2048,8192]{1,0} parameter(14), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_15 = pred[128,2048]{1,0} parameter(15), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_16 = f32[128,8192]{1,0} parameter(16), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_17 = pred[128,8192]{1,0} parameter(17), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_18 = f32[128,2048]{1,0} parameter(18), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_19 = f32[8192,2048]{1,0} parameter(19), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.20 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[2048]{0} %param_4, /*index=5*/f32[8192,2048]{1,0} %param_5, f32[8192]{0} %param_6, f32[2048,8192]{1,0} %param_7, f32[128,2048]{1,0} %param_8, pred[128,2048]{1,0} %param_9, /*index=10*/f32[128,8192]{1,0} %param_10, pred[128,8192]{1,0} %param_11, f32[8192,2048]{1,0} %param_12, f32[128,2048]{1,0} %param_13, f32[2048,8192]{1,0} %param_14, /*index=15*/pred[128,2048]{1,0} %param_15, f32[128,8192]{1,0} %param_16, pred[128,8192]{1,0} %param_17, f32[128,2048]{1,0} %param_18, f32[8192,2048]{1,0} %param_19)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.280 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.289 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.288 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.83 = f32[] constant(3.81469727e-06)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.109 = f32[128,2048]{1,0} broadcast(f32[] %constant.83), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.51 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %get-tuple-element.288, f32[128,2048]{1,0} %broadcast.109), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.84 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.110 = f32[128,2048]{1,0} broadcast(f32[] %constant.84), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.6 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.289, f32[128,2048]{1,0} %multiply.51, f32[128,2048]{1,0} %broadcast.110), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.85 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.4 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.6, f32[] %constant.85), dimensions={0}, to_apply=%region_0.300.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.51 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.280, f32[2048]{0} %reduce.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.281 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.290 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=10
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.15 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.6, f32[128,8192]{1,0} %get-tuple-element.290), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.4 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.15), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.52 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.281, f32[8192,2048]{0,1} %transpose.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.282 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.291 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=11
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.292 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=12
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.16 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.6, f32[8192,2048]{1,0} %get-tuple-element.292), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.86 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.111 = f32[128,8192]{1,0} broadcast(f32[] %constant.86), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.7 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.291, f32[128,8192]{1,0} %dot.16, f32[128,8192]{1,0} %broadcast.111), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.5 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.7, f32[] %constant.85), dimensions={0}, to_apply=%region_1.304.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.53 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.282, f32[8192]{0} %reduce.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.283 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.293 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=13
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.17 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.7, f32[128,2048]{1,0} %get-tuple-element.293), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.5 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.17), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.54 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.283, f32[2048,8192]{0,1} %transpose.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.294 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=14
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.18 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %select.7, f32[2048,8192]{1,0} %get-tuple-element.294), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.21 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %add.51, f32[8192,2048]{1,0} %add.52, f32[8192]{0} %add.53, f32[2048,8192]{1,0} %add.54, f32[128,2048]{1,0} %dot.18)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.46 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.47 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.48 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.49 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.284 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.295 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=15
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.50 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.87 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.112 = f32[128,2048]{1,0} broadcast(f32[] %constant.87), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.8 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.295, f32[128,2048]{1,0} %get-tuple-element.50, f32[128,2048]{1,0} %broadcast.112), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.88 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.6 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.8, f32[] %constant.88), dimensions={0}, to_apply=%region_2.343.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.55 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.284, f32[2048]{0} %reduce.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.285 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.296 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=16
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.19 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.8, f32[128,8192]{1,0} %get-tuple-element.296), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.6 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.19), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.56 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.285, f32[8192,2048]{0,1} %transpose.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.286 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.297 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=17
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.299 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=19
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.20 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.8, f32[8192,2048]{1,0} %get-tuple-element.299), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.89 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.113 = f32[128,8192]{1,0} broadcast(f32[] %constant.89), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.9 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.297, f32[128,8192]{1,0} %dot.20, f32[128,8192]{1,0} %broadcast.113), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.7 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.9, f32[] %constant.88), dimensions={0}, to_apply=%region_3.347.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.57 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.286, f32[8192]{0} %reduce.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.287 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.298 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=18
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.21 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.9, f32[128,2048]{1,0} %get-tuple-element.298), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.7 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.21), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.58 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.287, f32[2048,8192]{0,1} %transpose.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.22 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %add.55, f32[8192,2048]{1,0} %add.56, f32[8192]{0} %add.57, f32[2048,8192]{1,0} %add.58)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.51 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.52 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.53 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.54 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.23 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.46, f32[8192,2048]{1,0} %get-tuple-element.47, f32[8192]{0} %get-tuple-element.48, f32[2048,8192]{1,0} %get-tuple-element.49, f32[2048]{0} %get-tuple-element.51, /*index=5*/f32[8192,2048]{1,0} %get-tuple-element.52, f32[8192]{0} %get-tuple-element.53, f32[2048,8192]{1,0} %get-tuple-element.54)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.415 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.416 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.417 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.418 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.419 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.420 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.421 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.422 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.24 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.415, f32[8192,2048]{1,0} %get-tuple-element.416, f32[8192]{0} %get-tuple-element.417, f32[2048,8192]{1,0} %get-tuple-element.418, f32[2048]{0} %get-tuple-element.419, /*index=5*/f32[8192,2048]{1,0} %get-tuple-element.420, f32[8192]{0} %get-tuple-element.421, f32[2048,8192]{1,0} %get-tuple-element.422), metadata={op_name="tuple.9"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.459 GB, invar_size=0.451 GB, outvar_size=0.250 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (False, False, False, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_1-stage_0_1_acc_grad_0, entry_computation_layout={(f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0},f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.1125-stage_0_1_acc_grad_0 (param_0: f32[2048,8192], param_1: f32[8192], param_2: f32[8192,2048], param_3: f32[2048], param_4: f32[128,2048], param_5: f32[2048,8192], param_6: f32[8192], param_7: f32[8192,2048], param_8: f32[2048], param_9: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192], f32[128,2048], /*index=5*/pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048,8192]{1,0} parameter(0), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192]{0} parameter(1), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192,2048]{1,0} parameter(2), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048]{0} parameter(3), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[2048,8192]{1,0} parameter(5), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[8192]{0} parameter(6), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = f32[8192,2048]{1,0} parameter(7), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[2048]{0} parameter(8), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[128,2048]{1,0} parameter(9), metadata={op_name="stage_0_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.13 = (f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) tuple(f32[2048,8192]{1,0} %param_0, f32[8192]{0} %param_1, f32[8192,2048]{1,0} %param_2, f32[2048]{0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/f32[2048,8192]{1,0} %param_5, f32[8192]{0} %param_6, f32[8192,2048]{1,0} %param_7, f32[2048]{0} %param_8, f32[128,2048]{1,0} %param_9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.78 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.74 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.11 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.78, f32[2048,8192]{1,0} %get-tuple-element.74), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.75 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.8 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.75), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.90 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.8), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.9 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.90), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.92 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.9), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.47 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.11, f32[128,8192]{1,0} %broadcast.92), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_0/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.78 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.94 = f32[128,8192]{1,0} broadcast(f32[] %constant.78), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.4 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.47, f32[128,8192]{1,0} %broadcast.94), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.76 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.12 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.4, f32[8192,2048]{1,0} %get-tuple-element.76), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.77 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.10 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.77), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.95 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.10), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.11 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.95), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.97 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.11), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.48 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.12, f32[128,2048]{1,0} %broadcast.97), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/Dense_1/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.79 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.99 = f32[128,2048]{1,0} broadcast(f32[] %constant.79), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.5 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.48, f32[128,2048]{1,0} %broadcast.99), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.6 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.48, f32[128,2048]{1,0} %broadcast.99), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.7 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.47, f32[128,8192]{1,0} %broadcast.94), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.14 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %maximum.5, pred[128,2048]{1,0} %compare.6, f32[128,8192]{1,0} %maximum.4, pred[128,8192]{1,0} %compare.7)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.10 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.11 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.12 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.13 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.14), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.79 = f32[2048,8192]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.13 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.10, f32[2048,8192]{1,0} %get-tuple-element.79), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.80 = f32[8192]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.12 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.80), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.100 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.12), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.13 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.100), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.101 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.13), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.49 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.13, f32[128,8192]{1,0} %broadcast.101), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.80 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.102 = f32[128,8192]{1,0} broadcast(f32[] %constant.80), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.6 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.49, f32[128,8192]{1,0} %broadcast.102), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.81 = f32[8192,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.14 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.6, f32[8192,2048]{1,0} %get-tuple-element.81), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.82 = f32[2048]{0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.14 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.82), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.103 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.14), dimensions={0,1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.15 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.103), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.105 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.15), dimensions={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.50 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.14, f32[128,2048]{1,0} %broadcast.105), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.81 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.107 = f32[128,2048]{1,0} broadcast(f32[] %constant.81), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.7 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.50, f32[128,2048]{1,0} %broadcast.107), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.83 = f32[128,2048]{1,0} get-tuple-element((f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}, /*index=5*/f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, f32[128,2048]{1,0}) %tuple.13), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %subtract.5 = f32[128,2048]{1,0} subtract(f32[128,2048]{1,0} %maximum.7, f32[128,2048]{1,0} %get-tuple-element.83), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.82 = f32[] constant(2)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.108 = f32[128,2048]{1,0} broadcast(f32[] %constant.82), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.50 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %subtract.5, f32[128,2048]{1,0} %broadcast.108), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.8 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.50, f32[128,2048]{1,0} %broadcast.107), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.9 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.49, f32[128,8192]{1,0} %broadcast.102), direction=GT, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_0)/jit(stage_0_1_acc_grad_01)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.15 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %multiply.50, pred[128,2048]{1,0} %compare.8, f32[128,8192]{1,0} %maximum.6, pred[128,8192]{1,0} %compare.9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.14 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.15 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.16 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.17 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.15), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.10, pred[128,2048]{1,0} %get-tuple-element.11, f32[128,8192]{1,0} %get-tuple-element.12, pred[128,8192]{1,0} %get-tuple-element.13, f32[128,2048]{1,0} %get-tuple-element.14, /*index=5*/pred[128,2048]{1,0} %get-tuple-element.15, f32[128,8192]{1,0} %get-tuple-element.16, pred[128,8192]{1,0} %get-tuple-element.17)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.166 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.167 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.168 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.169 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.170 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.171 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.172 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.173 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.17 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.166, pred[128,2048]{1,0} %get-tuple-element.167, f32[128,8192]{1,0} %get-tuple-element.168, pred[128,8192]{1,0} %get-tuple-element.169, f32[128,2048]{1,0} %get-tuple-element.170, /*index=5*/pred[128,2048]{1,0} %get-tuple-element.171, f32[128,8192]{1,0} %get-tuple-element.172, pred[128,8192]{1,0} %get-tuple-element.173), metadata={op_name="tuple.9"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.264 GB, invar_size=0.252 GB, outvar_size=0.012 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[128,2048]), ShapedArray(float32[8192,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_0_1-stage_0_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[8192,2048]{1,0},f32[128,2048]{1,0},f32[2048,8192]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[128,2048]{1,0},f32[8192,2048]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.304.stage_0_1_acc_grad_1 (Arg_0.305: f32[], Arg_1.306: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.305 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.306 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.307 = f32[] add(f32[] %Arg_0.305, f32[] %Arg_1.306), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.300.stage_0_1_acc_grad_1 (Arg_0.301: f32[], Arg_1.302: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.301 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.302 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.303 = f32[] add(f32[] %Arg_0.301, f32[] %Arg_1.302), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_3.347.stage_0_1_acc_grad_1 (Arg_0.348: f32[], Arg_1.349: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.348 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.349 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.350 = f32[] add(f32[] %Arg_0.348, f32[] %Arg_1.349), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_2.343.stage_0_1_acc_grad_1 (Arg_0.344: f32[], Arg_1.345: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.344 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.345 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.346 = f32[] add(f32[] %Arg_0.344, f32[] %Arg_1.345), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.1125-stage_0_1_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[2048], param_5: f32[8192,2048], param_6: f32[8192], param_7: f32[2048,8192], param_8: f32[128,2048], param_9: pred[128,2048], param_10: f32[128,8192], param_11: pred[128,8192], param_12: f32[8192,2048], param_13: f32[128,2048], param_14: f32[2048,8192], param_15: pred[128,2048], param_16: f32[128,8192], param_17: pred[128,8192], param_18: f32[128,2048], param_19: f32[8192,2048]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192], f32[2048], /*index=5*/f32[8192,2048], f32[8192], f32[2048,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[2048]{0} parameter(4), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[8192,2048]{1,0} parameter(5), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[8192]{0} parameter(6), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = f32[2048,8192]{1,0} parameter(7), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[128,2048]{1,0} parameter(8), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = pred[128,2048]{1,0} parameter(9), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_10 = f32[128,8192]{1,0} parameter(10), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_11 = pred[128,8192]{1,0} parameter(11), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_12 = f32[8192,2048]{1,0} parameter(12), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_13 = f32[128,2048]{1,0} parameter(13), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_14 = f32[2048,8192]{1,0} parameter(14), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_15 = pred[128,2048]{1,0} parameter(15), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_16 = f32[128,8192]{1,0} parameter(16), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_17 = pred[128,8192]{1,0} parameter(17), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_18 = f32[128,2048]{1,0} parameter(18), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_19 = f32[8192,2048]{1,0} parameter(19), metadata={op_name="stage_0_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.20 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[2048]{0} %param_4, /*index=5*/f32[8192,2048]{1,0} %param_5, f32[8192]{0} %param_6, f32[2048,8192]{1,0} %param_7, f32[128,2048]{1,0} %param_8, pred[128,2048]{1,0} %param_9, /*index=10*/f32[128,8192]{1,0} %param_10, pred[128,8192]{1,0} %param_11, f32[8192,2048]{1,0} %param_12, f32[128,2048]{1,0} %param_13, f32[2048,8192]{1,0} %param_14, /*index=15*/pred[128,2048]{1,0} %param_15, f32[128,8192]{1,0} %param_16, pred[128,8192]{1,0} %param_17, f32[128,2048]{1,0} %param_18, f32[8192,2048]{1,0} %param_19)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.280 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.289 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.288 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.83 = f32[] constant(3.81469727e-06)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.109 = f32[128,2048]{1,0} broadcast(f32[] %constant.83), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.51 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %get-tuple-element.288, f32[128,2048]{1,0} %broadcast.109), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.84 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.110 = f32[128,2048]{1,0} broadcast(f32[] %constant.84), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.6 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.289, f32[128,2048]{1,0} %multiply.51, f32[128,2048]{1,0} %broadcast.110), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.85 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.4 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.6, f32[] %constant.85), dimensions={0}, to_apply=%region_0.300.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.51 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.280, f32[2048]{0} %reduce.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.281 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.290 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=10
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.15 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.6, f32[128,8192]{1,0} %get-tuple-element.290), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.4 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.15), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.52 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.281, f32[8192,2048]{0,1} %transpose.4), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.282 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.291 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=11
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.292 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=12
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.16 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.6, f32[8192,2048]{1,0} %get-tuple-element.292), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.86 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.111 = f32[128,8192]{1,0} broadcast(f32[] %constant.86), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.7 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.291, f32[128,8192]{1,0} %dot.16, f32[128,8192]{1,0} %broadcast.111), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.5 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.7, f32[] %constant.85), dimensions={0}, to_apply=%region_1.304.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.53 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.282, f32[8192]{0} %reduce.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.283 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.293 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=13
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.17 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.7, f32[128,2048]{1,0} %get-tuple-element.293), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.5 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.17), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.54 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.283, f32[2048,8192]{0,1} %transpose.5), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.294 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=14
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.18 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %select.7, f32[2048,8192]{1,0} %get-tuple-element.294), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.21 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %add.51, f32[8192,2048]{1,0} %add.52, f32[8192]{0} %add.53, f32[2048,8192]{1,0} %add.54, f32[128,2048]{1,0} %dot.18)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.46 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.47 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.48 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.49 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.284 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.295 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=15
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.50 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.21), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.87 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.112 = f32[128,2048]{1,0} broadcast(f32[] %constant.87), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.8 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.295, f32[128,2048]{1,0} %get-tuple-element.50, f32[128,2048]{1,0} %broadcast.112), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=71}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.88 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.6 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.8, f32[] %constant.88), dimensions={0}, to_apply=%region_2.343.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.55 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.284, f32[2048]{0} %reduce.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.285 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.296 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=16
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.19 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.8, f32[128,8192]{1,0} %get-tuple-element.296), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.6 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.19), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.56 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.285, f32[8192,2048]{0,1} %transpose.6), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.286 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.297 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=17
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.299 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=19
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.20 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.8, f32[8192,2048]{1,0} %get-tuple-element.299), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.89 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.113 = f32[128,8192]{1,0} broadcast(f32[] %constant.89), dimensions={}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.9 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.297, f32[128,8192]{1,0} %dot.20, f32[128,8192]{1,0} %broadcast.113), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=69}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.7 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.9, f32[] %constant.88), dimensions={0}, to_apply=%region_3.347.stage_0_1_acc_grad_1, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.57 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.286, f32[8192]{0} %reduce.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.287 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.298 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, pred[128,2048]{1,0}, /*index=10*/f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, f32[2048,8192]{1,0}, /*index=15*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[128,2048]{1,0}, f32[8192,2048]{1,0}) %tuple.20), index=18
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.21 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.9, f32[128,2048]{1,0} %get-tuple-element.298), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.7 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.21), dimensions={1,0}, metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/transpose(jvp(MLPModel))/Dense_0/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.58 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.287, f32[2048,8192]{0,1} %transpose.7), metadata={op_name="parallelize(stage_0_1)/jit(main)/jit(stage_0_1_acc_grad_1)/jit(stage_0_1_acc_grad_11)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.22 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %add.55, f32[8192,2048]{1,0} %add.56, f32[8192]{0} %add.57, f32[2048,8192]{1,0} %add.58)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.51 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.52 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.53 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.54 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.22), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.23 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.46, f32[8192,2048]{1,0} %get-tuple-element.47, f32[8192]{0} %get-tuple-element.48, f32[2048,8192]{1,0} %get-tuple-element.49, f32[2048]{0} %get-tuple-element.51, /*index=5*/f32[8192,2048]{1,0} %get-tuple-element.52, f32[8192]{0} %get-tuple-element.53, f32[2048,8192]{1,0} %get-tuple-element.54)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.415 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.416 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.417 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.418 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.419 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.420 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.421 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.422 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) %tuple.23), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.24 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[2048]{0}, /*index=5*/f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}) tuple(f32[2048]{0} %get-tuple-element.415, f32[8192,2048]{1,0} %get-tuple-element.416, f32[8192]{0} %get-tuple-element.417, f32[2048,8192]{1,0} %get-tuple-element.418, f32[2048]{0} %get-tuple-element.419, /*index=5*/f32[8192,2048]{1,0} %get-tuple-element.420, f32[8192]{0} %get-tuple-element.421, f32[2048,8192]{1,0} %get-tuple-element.422), metadata={op_name="tuple.9"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.459 GB, invar_size=0.451 GB, outvar_size=0.250 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_1_1-stage_1_1_acc_grad_0, entry_computation_layout={(f32[128,2048]{1,0},f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.609-stage_1_1_acc_grad_0 (param_0: f32[128,2048], param_1: f32[2048,8192], param_2: f32[8192], param_3: f32[8192,2048], param_4: f32[2048], param_5: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[128,2048]{1,0} parameter(0), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[2048,8192]{1,0} parameter(1), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[8192,2048]{1,0} parameter(3), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[2048]{0} parameter(4), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[128,2048]{1,0} parameter(5), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.10 = (f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) tuple(f32[128,2048]{1,0} %param_0, f32[2048,8192]{1,0} %param_1, f32[8192]{0} %param_2, f32[8192,2048]{1,0} %param_3, f32[2048]{0} %param_4, /*index=5*/f32[128,2048]{1,0} %param_5)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.45 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.46 = f32[2048,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.6 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.45, f32[2048,8192]{1,0} %get-tuple-element.46), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.47 = f32[8192]{0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.4 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.47), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.46 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.4), dimensions={0,1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.5 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.46), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.47 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.5), dimensions={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.23 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.6, f32[128,8192]{1,0} %broadcast.47), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.40 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.48 = f32[128,8192]{1,0} broadcast(f32[] %constant.40), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.2 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.23, f32[128,8192]{1,0} %broadcast.48), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.48 = f32[8192,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.7 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.2, f32[8192,2048]{1,0} %get-tuple-element.48), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.49 = f32[2048]{0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.6 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.49), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.49 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.6), dimensions={0,1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.7 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.49), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.50 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.7), dimensions={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.24 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.7, f32[128,2048]{1,0} %broadcast.50), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.41 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.51 = f32[128,2048]{1,0} broadcast(f32[] %constant.41), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.3 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.24, f32[128,2048]{1,0} %broadcast.51), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.50 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %subtract.3 = f32[128,2048]{1,0} subtract(f32[128,2048]{1,0} %maximum.3, f32[128,2048]{1,0} %get-tuple-element.50), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.42 = f32[] constant(2)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.52 = f32[128,2048]{1,0} broadcast(f32[] %constant.42), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.26 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %subtract.3, f32[128,2048]{1,0} %broadcast.52), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.3 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.24, f32[128,2048]{1,0} %broadcast.51), direction=GT, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.4 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.23, f32[128,8192]{1,0} %broadcast.48), direction=GT, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.11 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %multiply.26, pred[128,2048]{1,0} %compare.3, f32[128,8192]{1,0} %maximum.2, pred[128,8192]{1,0} %compare.4)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.6 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.7 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.8 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.9 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.12 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.6, pred[128,2048]{1,0} %get-tuple-element.7, f32[128,8192]{1,0} %get-tuple-element.8, pred[128,8192]{1,0} %get-tuple-element.9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.97 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.98 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.99 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.100 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.13 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.97, pred[128,2048]{1,0} %get-tuple-element.98, f32[128,8192]{1,0} %get-tuple-element.99, pred[128,8192]{1,0} %get-tuple-element.100), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.133 GB, invar_size=0.127 GB, outvar_size=0.006 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_1_1-stage_1_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[8192,2048]{1,0},f32[128,2048]{1,0},f32[2048,8192]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.179.stage_1_1_acc_grad_1 (Arg_0.180: f32[], Arg_1.181: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.180 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.181 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.182 = f32[] add(f32[] %Arg_0.180, f32[] %Arg_1.181), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.175.stage_1_1_acc_grad_1 (Arg_0.176: f32[], Arg_1.177: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.176 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.177 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.178 = f32[] add(f32[] %Arg_0.176, f32[] %Arg_1.177), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.609-stage_1_1_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[128,2048], param_5: pred[128,2048], param_6: f32[128,8192], param_7: pred[128,8192], param_8: f32[8192,2048], param_9: f32[128,2048], param_10: f32[2048,8192]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192], f32[128,2048]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = pred[128,2048]{1,0} parameter(5), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[128,8192]{1,0} parameter(6), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = pred[128,8192]{1,0} parameter(7), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[8192,2048]{1,0} parameter(8), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[128,2048]{1,0} parameter(9), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_10 = f32[2048,8192]{1,0} parameter(10), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/pred[128,2048]{1,0} %param_5, f32[128,8192]{1,0} %param_6, pred[128,8192]{1,0} %param_7, f32[8192,2048]{1,0} %param_8, f32[128,2048]{1,0} %param_9, /*index=10*/f32[2048,8192]{1,0} %param_10)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.164 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.169 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.168 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.43 = f32[] constant(3.81469727e-06)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.53 = f32[128,2048]{1,0} broadcast(f32[] %constant.43), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.27 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %get-tuple-element.168, f32[128,2048]{1,0} %broadcast.53), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.44 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.54 = f32[128,2048]{1,0} broadcast(f32[] %constant.44), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.3 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.169, f32[128,2048]{1,0} %multiply.27, f32[128,2048]{1,0} %broadcast.54), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.45 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.2 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.3, f32[] %constant.45), dimensions={0}, to_apply=%region_0.175.stage_1_1_acc_grad_1, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.25 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.164, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.165 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.170 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.8 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[128,8192]{1,0} %get-tuple-element.170), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.2 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.8), dimensions={1,0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.26 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.165, f32[8192,2048]{0,1} %transpose.2), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.166 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.171 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.172 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.9 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[8192,2048]{1,0} %get-tuple-element.172), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.46 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.55 = f32[128,8192]{1,0} broadcast(f32[] %constant.46), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.4 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.171, f32[128,8192]{1,0} %dot.9, f32[128,8192]{1,0} %broadcast.55), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.3 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.4, f32[] %constant.45), dimensions={0}, to_apply=%region_1.179.stage_1_1_acc_grad_1, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.27 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.166, f32[8192]{0} %reduce.3), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.167 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.173 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.10 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[128,2048]{1,0} %get-tuple-element.173), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.3 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.10), dimensions={1,0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.28 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.167, f32[2048,8192]{0,1} %transpose.3), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.174 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=10
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.11 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[2048,8192]{1,0} %get-tuple-element.174), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.17 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %add.25, f32[8192,2048]{1,0} %add.26, f32[8192]{0} %add.27, f32[2048,8192]{1,0} %add.28, f32[128,2048]{1,0} %dot.11)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.25 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.26 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.27 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.28 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.29 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.18 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %get-tuple-element.25, f32[8192,2048]{1,0} %get-tuple-element.26, f32[8192]{0} %get-tuple-element.27, f32[2048,8192]{1,0} %get-tuple-element.28, f32[128,2048]{1,0} %get-tuple-element.29)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.238 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.239 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.240 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.241 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.242 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.19 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %get-tuple-element.238, f32[8192,2048]{1,0} %get-tuple-element.239, f32[8192]{0} %get-tuple-element.240, f32[2048,8192]{1,0} %get-tuple-element.241, f32[128,2048]{1,0} %get-tuple-element.242), metadata={op_name="tuple.6"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.266 GB, invar_size=0.257 GB, outvar_size=0.126 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]), ShapedArray(float32[8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[2048]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_1_1-stage_1_1_acc_grad_0, entry_computation_layout={(f32[128,2048]{1,0},f32[2048,8192]{1,0},f32[8192]{0},f32[8192,2048]{1,0},f32[2048]{0},f32[128,2048]{1,0})->(f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.609-stage_1_1_acc_grad_0 (param_0: f32[128,2048], param_1: f32[2048,8192], param_2: f32[8192], param_3: f32[8192,2048], param_4: f32[2048], param_5: f32[128,2048]) -> (f32[128,2048], pred[128,2048], f32[128,8192], pred[128,8192]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[128,2048]{1,0} parameter(0), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[2048,8192]{1,0} parameter(1), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[8192,2048]{1,0} parameter(3), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[2048]{0} parameter(4), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = f32[128,2048]{1,0} parameter(5), metadata={op_name="stage_1_1_acc_grad_0$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.10 = (f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) tuple(f32[128,2048]{1,0} %param_0, f32[2048,8192]{1,0} %param_1, f32[8192]{0} %param_2, f32[8192,2048]{1,0} %param_3, f32[2048]{0} %param_4, /*index=5*/f32[128,2048]{1,0} %param_5)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.45 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.46 = f32[2048,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.6 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %get-tuple-element.45, f32[2048,8192]{1,0} %get-tuple-element.46), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.47 = f32[8192]{0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.4 = f32[1,8192]{1,0} reshape(f32[8192]{0} %get-tuple-element.47), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/reshape[new_sizes=(1, 8192) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.46 = f32[1,8192]{1,0} broadcast(f32[1,8192]{1,0} %reshape.4), dimensions={0,1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.5 = f32[8192]{0} reshape(f32[1,8192]{1,0} %broadcast.46), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.47 = f32[128,8192]{1,0} broadcast(f32[8192]{0} %reshape.5), dimensions={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.23 = f32[128,8192]{1,0} add(f32[128,8192]{1,0} %dot.6, f32[128,8192]{1,0} %broadcast.47), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_2/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.40 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.48 = f32[128,8192]{1,0} broadcast(f32[] %constant.40), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.2 = f32[128,8192]{1,0} maximum(f32[128,8192]{1,0} %add.23, f32[128,8192]{1,0} %broadcast.48), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.48 = f32[8192,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.7 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %maximum.2, f32[8192,2048]{1,0} %get-tuple-element.48), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.49 = f32[2048]{0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.6 = f32[1,2048]{1,0} reshape(f32[2048]{0} %get-tuple-element.49), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/reshape[new_sizes=(1, 2048) dimensions=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.49 = f32[1,2048]{1,0} broadcast(f32[1,2048]{1,0} %reshape.6), dimensions={0,1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %reshape.7 = f32[2048]{0} reshape(f32[1,2048]{1,0} %broadcast.49), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.50 = f32[128,2048]{1,0} broadcast(f32[2048]{0} %reshape.7), dimensions={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.24 = f32[128,2048]{1,0} add(f32[128,2048]{1,0} %dot.7, f32[128,2048]{1,0} %broadcast.50), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/Dense_3/add" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.41 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.51 = f32[128,2048]{1,0} broadcast(f32[] %constant.41), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %maximum.3 = f32[128,2048]{1,0} maximum(f32[128,2048]{1,0} %add.24, f32[128,2048]{1,0} %broadcast.51), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/max" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.50 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, f32[2048,8192]{1,0}, f32[8192]{0}, f32[8192,2048]{1,0}, f32[2048]{0}, /*index=5*/f32[128,2048]{1,0}) %tuple.10), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %subtract.3 = f32[128,2048]{1,0} subtract(f32[128,2048]{1,0} %maximum.3, f32[128,2048]{1,0} %get-tuple-element.50), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/sub" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.42 = f32[] constant(2)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.52 = f32[128,2048]{1,0} broadcast(f32[] %constant.42), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.26 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %subtract.3, f32[128,2048]{1,0} %broadcast.52), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.3 = pred[128,2048]{1,0} compare(f32[128,2048]{1,0} %add.24, f32[128,2048]{1,0} %broadcast.51), direction=GT, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %compare.4 = pred[128,8192]{1,0} compare(f32[128,8192]{1,0} %add.23, f32[128,8192]{1,0} %broadcast.48), direction=GT, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_0)/jit(stage_1_1_acc_grad_00)/jvp(MLPModel)/gt" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.11 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %multiply.26, pred[128,2048]{1,0} %compare.3, f32[128,8192]{1,0} %maximum.2, pred[128,8192]{1,0} %compare.4)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.6 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.7 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.8 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.9 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.11), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.12 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.6, pred[128,2048]{1,0} %get-tuple-element.7, f32[128,8192]{1,0} %get-tuple-element.8, pred[128,8192]{1,0} %get-tuple-element.9)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.97 = f32[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.98 = pred[128,2048]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.99 = f32[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.100 = pred[128,8192]{1,0} get-tuple-element((f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) %tuple.12), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.13 = (f32[128,2048]{1,0}, pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}) tuple(f32[128,2048]{1,0} %get-tuple-element.97, pred[128,2048]{1,0} %get-tuple-element.98, f32[128,8192]{1,0} %get-tuple-element.99, pred[128,8192]{1,0} %get-tuple-element.100), metadata={op_name="tuple.5"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.133 GB, invar_size=0.127 GB, outvar_size=0.006 GB, temp_buffer_size=0.000 GB, available_memory=20.882 GB)
[2m[36m(ProfileWorker pid=1524906)[0m input_vars:  (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]), ShapedArray(bool[128,2048]), ShapedArray(float32[128,8192]), ShapedArray(bool[128,8192]), ShapedArray(float32[8192,2048]), ShapedArray(float32[128,2048]), ShapedArray(float32[2048,8192]))
[2m[36m(ProfileWorker pid=1524906)[0m output_avals (ShapedArray(float32[2048]), ShapedArray(float32[8192,2048]), ShapedArray(float32[8192]), ShapedArray(float32[2048,8192]), ShapedArray(float32[128,2048]))
[2m[36m(ProfileWorker pid=1524906)[0m donated_invars (True, True, True, True, False, False, False, False, False, False, False)
[2m[36m(ProfileWorker pid=1524906)[0m hlo module: HloModule stage_1_1-stage_1_1_acc_grad_1, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias) }, entry_computation_layout={(f32[2048]{0},f32[8192,2048]{1,0},f32[8192]{0},f32[2048,8192]{1,0},f32[128,2048]{1,0},pred[128,2048]{1,0},f32[128,8192]{1,0},pred[128,8192]{1,0},f32[8192,2048]{1,0},f32[128,2048]{1,0},f32[2048,8192]{1,0})->(f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0})}
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_1.179.stage_1_1_acc_grad_1 (Arg_0.180: f32[], Arg_1.181: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.180 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.181 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.182 = f32[] add(f32[] %Arg_0.180, f32[] %Arg_1.181), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m %region_0.175.stage_1_1_acc_grad_1 (Arg_0.176: f32[], Arg_1.177: f32[]) -> f32[] {
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_0.176 = f32[] parameter(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %Arg_1.177 = f32[] parameter(1)
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %add.178 = f32[] add(f32[] %Arg_0.176, f32[] %Arg_1.177), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m ENTRY %main.609-stage_1_1_acc_grad_1 (param_0: f32[2048], param_1: f32[8192,2048], param_2: f32[8192], param_3: f32[2048,8192], param_4: f32[128,2048], param_5: pred[128,2048], param_6: f32[128,8192], param_7: pred[128,8192], param_8: f32[8192,2048], param_9: f32[128,2048], param_10: f32[2048,8192]) -> (f32[2048], f32[8192,2048], f32[8192], f32[2048,8192], f32[128,2048]) {
[2m[36m(ProfileWorker pid=1524906)[0m   %param_0 = f32[2048]{0} parameter(0), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_1 = f32[8192,2048]{1,0} parameter(1), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_2 = f32[8192]{0} parameter(2), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_3 = f32[2048,8192]{1,0} parameter(3), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_4 = f32[128,2048]{1,0} parameter(4), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_5 = pred[128,2048]{1,0} parameter(5), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_6 = f32[128,8192]{1,0} parameter(6), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_7 = pred[128,8192]{1,0} parameter(7), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_8 = f32[8192,2048]{1,0} parameter(8), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_9 = f32[128,2048]{1,0} parameter(9), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %param_10 = f32[2048,8192]{1,0} parameter(10), metadata={op_name="stage_1_1_acc_grad_1$start"}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.16 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) tuple(f32[2048]{0} %param_0, f32[8192,2048]{1,0} %param_1, f32[8192]{0} %param_2, f32[2048,8192]{1,0} %param_3, f32[128,2048]{1,0} %param_4, /*index=5*/pred[128,2048]{1,0} %param_5, f32[128,8192]{1,0} %param_6, pred[128,8192]{1,0} %param_7, f32[8192,2048]{1,0} %param_8, f32[128,2048]{1,0} %param_9, /*index=10*/f32[2048,8192]{1,0} %param_10)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.164 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.169 = pred[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=5
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.168 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.43 = f32[] constant(3.81469727e-06)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.53 = f32[128,2048]{1,0} broadcast(f32[] %constant.43), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %multiply.27 = f32[128,2048]{1,0} multiply(f32[128,2048]{1,0} %get-tuple-element.168, f32[128,2048]{1,0} %broadcast.53), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/mul" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=228}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.44 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.54 = f32[128,2048]{1,0} broadcast(f32[] %constant.44), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 2048) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.3 = f32[128,2048]{1,0} select(pred[128,2048]{1,0} %get-tuple-element.169, f32[128,2048]{1,0} %multiply.27, f32[128,2048]{1,0} %broadcast.54), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=75}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.45 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.2 = f32[2048]{0} reduce(f32[128,2048]{1,0} %select.3, f32[] %constant.45), dimensions={0}, to_apply=%region_0.175.stage_1_1_acc_grad_1, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.25 = f32[2048]{0} add(f32[2048]{0} %get-tuple-element.164, f32[2048]{0} %reduce.2), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.165 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.170 = f32[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=6
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.8 = f32[2048,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[128,8192]{1,0} %get-tuple-element.170), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.2 = f32[8192,2048]{0,1} transpose(f32[2048,8192]{1,0} %dot.8), dimensions={1,0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.26 = f32[8192,2048]{1,0} add(f32[8192,2048]{1,0} %get-tuple-element.165, f32[8192,2048]{0,1} %transpose.2), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.166 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.171 = pred[128,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=7
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.172 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=8
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.9 = f32[128,8192]{1,0} dot(f32[128,2048]{1,0} %select.3, f32[8192,2048]{1,0} %get-tuple-element.172), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %constant.46 = f32[] constant(0)
[2m[36m(ProfileWorker pid=1524906)[0m   %broadcast.55 = f32[128,8192]{1,0} broadcast(f32[] %constant.46), dimensions={}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/broadcast_in_dim[shape=(128, 8192) broadcast_dimensions=()]" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %select.4 = f32[128,8192]{1,0} select(pred[128,8192]{1,0} %get-tuple-element.171, f32[128,8192]{1,0} %dot.9, f32[128,8192]{1,0} %broadcast.55), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/select_n" source_file="/home/jingqi/alpa/tests/tutorial/pipeline_shard.py" source_line=73}
[2m[36m(ProfileWorker pid=1524906)[0m   %reduce.3 = f32[8192]{0} reduce(f32[128,8192]{1,0} %select.4, f32[] %constant.45), dimensions={0}, to_apply=%region_1.179.stage_1_1_acc_grad_1, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/reduce_sum[axes=(0,)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.27 = f32[8192]{0} add(f32[8192]{0} %get-tuple-element.166, f32[8192]{0} %reduce.3), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.167 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.173 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=9
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.10 = f32[8192,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[128,2048]{1,0} %get-tuple-element.173), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((0,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %transpose.3 = f32[2048,8192]{0,1} transpose(f32[8192,2048]{1,0} %dot.10), dimensions={1,0}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/transpose[permutation=(1, 0)]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %add.28 = f32[2048,8192]{1,0} add(f32[2048,8192]{1,0} %get-tuple-element.167, f32[2048,8192]{0,1} %transpose.3), metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/add"}
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.174 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}, /*index=5*/pred[128,2048]{1,0}, f32[128,8192]{1,0}, pred[128,8192]{1,0}, f32[8192,2048]{1,0}, f32[128,2048]{1,0}, /*index=10*/f32[2048,8192]{1,0}) %tuple.16), index=10
[2m[36m(ProfileWorker pid=1524906)[0m   %dot.11 = f32[128,2048]{1,0} dot(f32[128,8192]{1,0} %select.4, f32[2048,8192]{1,0} %get-tuple-element.174), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(stage_1_1)/jit(main)/jit(stage_1_1_acc_grad_1)/jit(stage_1_1_acc_grad_10)/transpose(jvp(MLPModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/jingqi/anaconda3/envs/ddl/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.17 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %add.25, f32[8192,2048]{1,0} %add.26, f32[8192]{0} %add.27, f32[2048,8192]{1,0} %add.28, f32[128,2048]{1,0} %dot.11)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.25 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.26 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.27 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.28 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.29 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.17), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   %tuple.18 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %get-tuple-element.25, f32[8192,2048]{1,0} %get-tuple-element.26, f32[8192]{0} %get-tuple-element.27, f32[2048,8192]{1,0} %get-tuple-element.28, f32[128,2048]{1,0} %get-tuple-element.29)
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.238 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=0
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.239 = f32[8192,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=1
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.240 = f32[8192]{0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=2
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.241 = f32[2048,8192]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=3
[2m[36m(ProfileWorker pid=1524906)[0m   %get-tuple-element.242 = f32[128,2048]{1,0} get-tuple-element((f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) %tuple.18), index=4
[2m[36m(ProfileWorker pid=1524906)[0m   ROOT %tuple.19 = (f32[2048]{0}, f32[8192,2048]{1,0}, f32[8192]{0}, f32[2048,8192]{1,0}, f32[128,2048]{1,0}) tuple(f32[2048]{0} %get-tuple-element.238, f32[8192,2048]{1,0} %get-tuple-element.239, f32[8192]{0} %get-tuple-element.240, f32[2048,8192]{1,0} %get-tuple-element.241, f32[128,2048]{1,0} %get-tuple-element.242), metadata={op_name="tuple.6"}
[2m[36m(ProfileWorker pid=1524906)[0m }
[2m[36m(ProfileWorker pid=1524906)[0m 
[2m[36m(ProfileWorker pid=1524906)[0m 
result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.266 GB, invar_size=0.257 GB, outvar_size=0.126 GB, temp_buffer_size=0.008 GB, available_memory=20.882 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.132 GB, invar_size=0.126 GB, outvar_size=0.006 GB, temp_buffer_size=0.000 GB, available_memory=21.105 GB)
Profiling for submesh 0 (1, 1) takes 11.23 seconds
--------------------------------------------------
Profile result saved to: profile-results-2023-11-14-03-06-12.npy
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0], [1]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
